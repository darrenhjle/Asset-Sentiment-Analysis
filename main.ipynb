{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "from bs4 import BeautifulSoup\n",
    "import tf_keras as keras\n",
    "import re\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Tokenizer, Summarizer and Sentiment Analysis models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\79860\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFPegasusForConditionalGeneration.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFPegasusForConditionalGeneration were not initialized from the PyTorch model and are newly initialized: ['model.encoder.embed_positions.weight', 'model.decoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer for the financial summarization model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"human-centered-summarization/financial-summarization-pegasus\")\n",
    "\n",
    "# Initialize the summarization model\n",
    "summarizer = TFAutoModelForSeq2SeqLM.from_pretrained(\"human-centered-summarization/financial-summarization-pegasus\")\n",
    "\n",
    "# Initialize the sentiment analysis pipeline using the FinBERT model\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"yiyanghkust/finbert-tone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tickers to be used in the sentiment analysis\n",
    "# Using the top 2 cryptocurrencies as an example\n",
    "tickers = ['BTC', 'ETH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search news URL method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find news articles for a given ticker\n",
    "def search_for_news_urls(ticker):\n",
    "    # Construct the Google search URL for Yahoo Finance news related to the ticker\n",
    "    search_url = f\"https://www.google.com/search?q=yahoo+finance+{ticker}&tbm=nws\"\n",
    "    \n",
    "    # Make a request to the search URL\n",
    "    response = requests.get(search_url)\n",
    "    \n",
    "    # Parse the HTML content of the response\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all anchor tags in the parsed HTML content\n",
    "    anchor_tags = soup.find_all('a')\n",
    "    \n",
    "    # Extract the href attributes from the anchor tags\n",
    "    hrefs = [link['href'] for link in anchor_tags]\n",
    "    \n",
    "    return hrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['BTC', 'ETH'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary where each ticker is mapped to its corresponding news article URLs\n",
    "# The search_for_news_urls function is called for each ticker to get the news URLs\n",
    "raw = {ticker: search_for_news_urls(ticker) for ticker in tickers}\n",
    "\n",
    "# Display the keys of the raw dictionary, which are the tickers\n",
    "raw.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words to exclude\n",
    "exclude = [\n",
    "    'maps',\n",
    "    'policies', \n",
    "    'preferences', \n",
    "    'accounts', \n",
    "    'support'\n",
    "]\n",
    "\n",
    "# Function to filter out unwanted URLs based on the list of excluded words\n",
    "# @param urls: List of URLs to filter\n",
    "# @param excludeList: List of words to exclude from URLs\n",
    "# @return: List of filtered URLs\n",
    "def filter(urls, excludeList):\n",
    "    filtered_urls = []\n",
    "    for url in urls:\n",
    "        # Check if the URL starts with 'https://' and does not contain any excluded words\n",
    "        if 'https://' in url and not any(word in url for word in excludeList):\n",
    "            # Extract the URL up to the first '&' character\n",
    "            cleaned_url = re.findall(r'(https?://\\S+)', url)[0].split('&')[0]\n",
    "            filtered_urls.append(cleaned_url)\n",
    "    # Remove duplicates by converting the list to a set and back to a list\n",
    "    return list(set(filtered_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BTC': ['https://sg.finance.yahoo.com/news/bitcoins-double-top-suggests-btc-062739635.html',\n",
       "  'https://finance.yahoo.com/news/bitcoin-signals-potential-bottom-market-053052099.html',\n",
       "  'https://www.google.com/search?q%3Dyahoo%2Bfinance%2BBTC%26tbm%3Dnws%26pccc%3D1',\n",
       "  'https://finance.yahoo.com/video/bitcoin-back-below-60k-look-133426663.html',\n",
       "  'https://sg.finance.yahoo.com/news/48-singapore-crypto-investors-double-130000395.html',\n",
       "  'https://finance.yahoo.com/news/bitcoin-is-having-its-worst-week-since-the-fall-of-ftx-153406320.html',\n",
       "  'https://finance.yahoo.com/news/bitcoin-price-today-down-58k-062123138.html',\n",
       "  'https://sg.finance.yahoo.com/news/bitcoin-summer-2024-expect-153233549.html',\n",
       "  'https://sg.finance.yahoo.com/news/bitcoin-could-reach-high-us-023053354.html',\n",
       "  'https://finance.yahoo.com/video/bitcoin-rebound-markets-recover-market-215949834.html',\n",
       "  'https://sg.finance.yahoo.com/news/bitcoin-halving-prices-another-high-074351948.html'],\n",
       " 'ETH': ['https://finance.yahoo.com/news/solana-reaches-high-against-ethereum-062439434.html',\n",
       "  'https://finance.yahoo.com/news/spot-ethereum-etfs-receive-inflows-221707679.html',\n",
       "  'https://finance.yahoo.com/video/stocks-little-changed-investors-await-173336341.html',\n",
       "  'https://sg.finance.yahoo.com/news/bitcoins-smarter-brother-octas-guide-020000956.html',\n",
       "  'https://finance.yahoo.com/news/3-reasons-ultimate-cryptocurrency-buy-080100540.html',\n",
       "  'https://finance.yahoo.com/news/jump-trading-allegedly-moves-29m-104938115.html',\n",
       "  'https://www.google.com/search?q%3Dyahoo%2Bfinance%2BETH%26tbm%3Dnws%26pccc%3D1',\n",
       "  'https://finance.yahoo.com/video/crypto-market-rebounds-global-sell-150121276.html',\n",
       "  'https://finance.yahoo.com/news/3-crypto-stocks-watch-ethereum-115600602.html',\n",
       "  'https://sg.finance.yahoo.com/news/ether-slides-20-trading-firm-050241805.html',\n",
       "  'https://sg.finance.yahoo.com/news/blackrock-seeing-only-little-bit-160509328.html']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the URLs for each ticker by removing unwanted URLs based on the exclude list\n",
    "# This creates a dictionary where each ticker is mapped to a list of cleaned URLs\n",
    "cleanURLS = {ticker: filter(raw[ticker], exclude) for ticker in raw.keys()}\n",
    "\n",
    "# Display the cleaned URLs for each ticker\n",
    "cleanURLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "# Function to scrape articles from a list of URLs\n",
    "# @param urls: List of URLs to scrape\n",
    "# @return: List of articles' text content\n",
    "def scrape(urls):\n",
    "    articles = []\n",
    "    \n",
    "    # Define the header to mimic a browser request\n",
    "    header = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    \n",
    "    for url in urls:\n",
    "        # Delay between requests for a random amount of time (1 to 5 seconds) to reduce the chances of being blocked\n",
    "        time.sleep(random.randint(1, 5))\n",
    "        \n",
    "        # Make a request to the URL with the defined header\n",
    "        response = requests.get(url, headers=header)\n",
    "        \n",
    "        # Parse the HTML content of the response\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find all paragraph tags in the parsed HTML content\n",
    "        paragraphs = soup.find_all('p')\n",
    "        \n",
    "        # Extract the text from each paragraph and join them into a single string\n",
    "        text = [paragraph.text for paragraph in paragraphs]\n",
    "        \n",
    "        # Limit the text to the first 350 words for summarization\n",
    "        words = ' '.join(text).split(' ')[:350]\n",
    "        article = ' '.join(words)\n",
    "        \n",
    "        # Append the cleaned article text to the articles list\n",
    "        articles.append(article)\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = {ticker: scrape(cleanURLS[ticker]) for ticker in tickers}\n",
    "articles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
